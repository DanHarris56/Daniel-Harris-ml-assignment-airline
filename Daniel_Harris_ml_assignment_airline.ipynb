{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsdGvsSvRfm"
      },
      "source": [
        "# Machine Learning Assignment\n",
        "\n",
        "**Dataset**:       AIRLINE SATISFACTION\n",
        "\n",
        "**Student ID**:    s5510805\n",
        "\n",
        "**Student Name**:  Daniel Harris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJlfhOtBvRfp"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Add imports here as needed.\n",
        "\n",
        "Remember to **re-run the cell when you add imports**, so it gets loaded into the virtual notebook environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-04T15:47:13.909935Z",
          "start_time": "2023-07-04T15:47:12.383188Z"
        },
        "id": "yjclbLFpvRfp"
      },
      "outputs": [],
      "source": [
        "# Data and Datasets\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Clustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Validation methods\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Imputing\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Hyper-parameter optimisation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Feature selection & feature engineering\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Stats\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.stats import shapiro     # Shapiro Wilk\n",
        "from scipy.stats import normaltest  # D’Agostino’s K^2\n",
        "from scipy.stats import anderson    # Anderson-Darling\n",
        "from scipy.stats import ttest_ind    # independent student t-test; assumes normality\n",
        "from scipy.stats import mannwhitneyu # non-parametric; doesn't assume normality\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from IPython.display import display\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "# Utils\n",
        "import pprint\n",
        "import numpy as np\n",
        "from time import time\n",
        "import openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nxr55TwvRfq"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-04T15:47:13.944669Z",
          "start_time": "2023-07-04T15:47:13.911885Z"
        },
        "id": "0VCjJ2SCvRfr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/airline-satisfaction.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChIslaVAvRft"
      },
      "source": [
        "## Task 2.1 - ML Workflow to Critically Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-04T15:47:13.957881Z",
          "start_time": "2023-07-04T15:47:13.946606Z"
        },
        "id": "sWRfMNrHvRft"
      },
      "outputs": [],
      "source": [
        "# Dropping all rows with missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Converting all non-numeric (object) features to numeric\n",
        "cat_columns = df.select_dtypes(['object']).columns\n",
        "df[cat_columns] = df[cat_columns].apply(lambda x: x.astype('category')) # converting 'object' columns to 'category' type\n",
        "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes) # converting the 'category' columns to integer encoded values\n",
        "\n",
        "# Splits the Pandas DataFrame into a feature matrix (X) and class/label vector (y)\n",
        "X = df.iloc[:,:len(df.columns)-1]\n",
        "y = df.iloc[:,len(df.columns)-1]\n",
        "\n",
        "# Splitting dataset for hold-out validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, # feature matrix\n",
        "                                                    y, # label vector\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=1,\n",
        "                                                    stratify=None\n",
        "                                                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-07-04T15:47:13.975210Z",
          "start_time": "2023-07-04T15:47:13.958596Z"
        },
        "id": "ALkGSp82vRft"
      },
      "outputs": [],
      "source": [
        "# Creating and testing a Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Testing the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Printing out performance of the model\n",
        "print(\"Accuracy: %s\" % (metrics.accuracy_score(y_test, y_pred)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2KVhxdvRfu"
      },
      "source": [
        "## Task 2.3 - Evaluation of Improved ML Workflow\n",
        "\n",
        "Add code for running your **improved** machine learning experiments below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Processing"
      ],
      "metadata": {
        "id": "411KJztVQAcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/airline-satisfaction.csv')\n",
        "# Missing Values Handling\n",
        "\n",
        "# Dropping rows with high missing values count and id column\n",
        "df = df.dropna(thresh=len(df.columns) - 4)\n",
        "df = df.drop(columns=['id'])\n",
        "\n",
        "# Impute missing values in 'type_of_travel' and 'travel_class' with 'Unassigned'\n",
        "for column in ['type_of_travel', 'travel_class']:\n",
        "  df[column].fillna(value='Unassigned', inplace=True)\n",
        "\n",
        "# K-NN Imputer for Rating Columns\n",
        "\n",
        "# Columns identified as having rating data\n",
        "rating_columns = ['inflight_wifi_service', 'online_boarding', 'inflight_entertainment']\n",
        "\n",
        "# Apply KNN imputation only to the rating columns\n",
        "df[rating_columns] = KNNImputer(n_neighbors=5).fit_transform(df[rating_columns])\n",
        "\n",
        "# Linear Regression Imputation\n",
        "\n",
        "# Dropping rows where both 'departure_delay_in_minutes' and 'arrival_delay_in_minutes' are missing\n",
        "df = df.dropna(subset=['departure_delay_in_minutes', 'arrival_delay_in_minutes'], how='all')\n",
        "\n",
        "def linear_regression_impute(df, target, predictor):\n",
        "  # Filter out rows where either target or predictor is NaN\n",
        "  not_null_df = df.dropna(subset=[target, predictor])\n",
        "\n",
        "  # Train a linear regression model\n",
        "  model = LinearRegression()\n",
        "  model.fit(not_null_df[[predictor]], not_null_df[target])\n",
        "\n",
        "  # Predict the missing values\n",
        "  predict_df = df[df[target].isnull() & df[predictor].notnull()]\n",
        "  predicted_values = model.predict(predict_df[[predictor]])\n",
        "\n",
        "  return predicted_values, predict_df.index\n",
        "\n",
        "# Impute 'departure_delay_in_minutes' using 'arrival_delay_in_minutes'\n",
        "departure_pred, departure_idx = linear_regression_impute(df, 'departure_delay_in_minutes', 'arrival_delay_in_minutes')\n",
        "df.loc[departure_idx, 'departure_delay_in_minutes'] = departure_pred\n",
        "\n",
        "# Dropping arrival_delay_in_minutes\n",
        "df = df.drop(columns=['arrival_delay_in_minutes'])\n",
        "\n",
        "# One-Hot Encoding of Categorical Variables\n",
        "\n",
        "# Exclude 'class' column from one-hot encoding\n",
        "cat_columns = df.select_dtypes(include=['object']).columns\n",
        "cat_columns = cat_columns[cat_columns != 'class']\n",
        "\n",
        "# Perform one-hot encoding\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded_features = encoder.fit_transform(df[cat_columns])\n",
        "encoded_feature_names = encoder.get_feature_names_out(cat_columns)\n",
        "\n",
        "# Create a DataFrame from the one-hot encoded features\n",
        "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)\n",
        "\n",
        "# Drop original categorical columns from the DataFrame\n",
        "df = df.drop(columns=cat_columns)\n",
        "\n",
        "# Concatenate the original DataFrame with the one-hot encoded DataFrame\n",
        "df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "# Convert float64 columns to int64 by rounding\n",
        "for column in df.columns:\n",
        "  if df[column].dtype == 'float64':\n",
        "    df[column] = df[column].round().astype('int64')\n",
        "\n",
        "# Encode the target variable 'class'\n",
        "df['class'] = LabelEncoder().fit_transform(df['class'])\n",
        "\n",
        "print(df.shape)\n"
      ],
      "metadata": {
        "id": "zgXH1lyVzPXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Anomaly Handling"
      ],
      "metadata": {
        "id": "P4bT2RF3h1Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anomaly Removal\n",
        "\n",
        "# Columns to check for 0 values\n",
        "columns_to_check = ['on_board_service', 'leg_room_service', 'cleanliness', 'inflight_service', 'food_and_drink']\n",
        "\n",
        "# Filter out rows where any of the specified columns have a 0 value\n",
        "df = df[~df[columns_to_check].isin([0]).any(axis=1)]\n",
        "\n",
        "# Remove 44 Anomaly and 999 Anomaly\n",
        "df = df[df['inflight_entertainment'] != 44]\n",
        "df = df[df['age'] != 999]\n",
        "\n",
        "# Filter out records where 'flight_distance' is greater than 10000\n",
        "df = df[df['flight_distance'] <= 10000]\n",
        "\n",
        "# Filter out records where 'departure_delay_in_minutes' is over 600\n",
        "df = df[df['departure_delay_in_minutes'] <= 600]\n",
        "\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('data/cleaned_airline_satisfaction_no_anomalies.csv')"
      ],
      "metadata": {
        "id": "nQ7ySTbhTTYT"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Train split"
      ],
      "metadata": {
        "id": "hQv4Sk5JE55s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the features and the target variable\n",
        "X = df.drop('class', axis=1)\n",
        "y = df['class']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
      ],
      "metadata": {
        "id": "dg_kpJyXYhAU"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SMOTE"
      ],
      "metadata": {
        "id": "j2FfaPgbYeib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to the training set\n",
        "smote = SMOTE()\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "tzw1x1xjYzFL"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standardization"
      ],
      "metadata": {
        "id": "fZUf7t98iAHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "eQMCJgJ3vRfu"
      },
      "outputs": [],
      "source": [
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('data/airline_satisfaction_standardized.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier Evaluation Utility Function"
      ],
      "metadata": {
        "id": "xklI7Tm2hDBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classifier(y_true, y_pred):\n",
        "    # Calculate the confusion matrix\n",
        "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
        "    print(\"Accuracy (Testing): %.2f\" % accuracy)\n",
        "\n",
        "    # Calculate macro-averaged precision and recall\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    print(\"Macro-Averaged Precision: %.2f\" % precision)\n",
        "    print(\"Macro-Averaged Recall: %.2f\" % recall)"
      ],
      "metadata": {
        "id": "JkhiZLv0g_xh"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "-tGko2ry5S5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and testing a Logistic Regression Model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_scaled, y_train_smote)\n",
        "\n",
        "# Testing the model\n",
        "y_pred_lr = model.predict(X_test_scaled)\n",
        "\n",
        "# Printing out performance of the model\n",
        "evaluate_classifier(y_test, y_pred_lr)"
      ],
      "metadata": {
        "id": "uglceCsI5SL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "ARNvzGqbFBO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK1bAnypvRfu"
      },
      "outputs": [],
      "source": [
        "# Create and train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier()\n",
        "rf_classifier.fit(X_train_scaled, y_train_smote)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "evaluate_classifier(y_test, y_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-NN"
      ],
      "metadata": {
        "id": "HfsJwNEbeM9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train K-NN Classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_classifier.fit(X_train_scaled, y_train_smote)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_knn = knn_classifier.predict(X_test_scaled)\n",
        "\n",
        "evaluate_classifier(y_test, y_pred_knn)"
      ],
      "metadata": {
        "id": "NiKwrzq5eMsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}